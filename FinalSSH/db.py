"""
db.py — SQLite data layer
=========================
Tables
------
  users            : tracks each user's current difficulty state
  questions        : question bank (Groq-generated questions are persisted here)
  interaction_logs : raw submission payloads + computed ΔD for audit/ML
"""

import json
import os
import sqlite3
import uuid

DB_PATH = os.path.join(os.path.dirname(__file__), "study_assistant.db")

# ─────────────────────────────────────────────────────────────────────────────
# Schema (DDL)
# ─────────────────────────────────────────────────────────────────────────────

SCHEMA = """
-- ── Users ──────────────────────────────────────────────────────────────────
CREATE TABLE IF NOT EXISTS users (
    user_id             TEXT    PRIMARY KEY,
    current_difficulty  REAL    NOT NULL DEFAULT 5.0,
    questions_answered  INTEGER NOT NULL DEFAULT 0,
    created_at          TEXT    NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at          TEXT    NOT NULL DEFAULT CURRENT_TIMESTAMP
);

-- ── Question Bank ───────────────────────────────────────────────────────────
-- Stores questions generated by Groq (or manually seeded).
-- `options` is JSON: {"A": "...", "B": "...", "C": "...", "D": "..."}
CREATE TABLE IF NOT EXISTS questions (
    question_id     TEXT    PRIMARY KEY,
    topic           TEXT    NOT NULL,
    question_text   TEXT    NOT NULL,
    options         TEXT    NOT NULL,   -- JSON blob
    correct_answer  TEXT    NOT NULL,   -- "A" | "B" | "C" | "D"
    difficulty      REAL    NOT NULL,   -- 0.0 – 10.0
    expected_time_ms INTEGER NOT NULL DEFAULT 25000,
    created_at      TEXT    NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX IF NOT EXISTS idx_questions_difficulty ON questions(difficulty);
CREATE INDEX IF NOT EXISTS idx_questions_topic      ON questions(topic);

-- ── Interaction Logs ────────────────────────────────────────────────────────
-- Raw submission payload + computed values, useful for ML/analytics.
CREATE TABLE IF NOT EXISTS interaction_logs (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id         TEXT    NOT NULL,
    question_id     TEXT    NOT NULL,
    payload         TEXT    NOT NULL,   -- full JSON from frontend
    delta_d         REAL,               -- computed ΔD
    new_difficulty  REAL,               -- D_{n+1}
    logged_at       TEXT    NOT NULL DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id)     REFERENCES users(user_id),
    FOREIGN KEY (question_id) REFERENCES questions(question_id)
);

CREATE INDEX IF NOT EXISTS idx_logs_user     ON interaction_logs(user_id);
CREATE INDEX IF NOT EXISTS idx_logs_question ON interaction_logs(question_id);

-- ── Assessment Sessions ───────────────────────────────────────────────────
-- Unified table for both Daily Quiz and AI Interview results.
CREATE TABLE IF NOT EXISTS assessment_sessions (
    session_id      TEXT    PRIMARY KEY,
    user_id         TEXT    NOT NULL,
    assessment_type TEXT    NOT NULL,   -- 'quiz' | 'interview'
    topic           TEXT    NOT NULL,
    score           REAL    NOT NULL,
    max_score       REAL    NOT NULL,
    percentage      REAL    NOT NULL,
    feedback        TEXT,
    metadata        TEXT,               -- JSON blob for extra details
    created_at      TEXT    NOT NULL DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (user_id) REFERENCES users(user_id)
);

CREATE INDEX IF NOT EXISTS idx_sessions_user ON assessment_sessions(user_id);
CREATE INDEX IF NOT EXISTS idx_sessions_topic ON assessment_sessions(topic);

-- ── Skill Metrics (Aggregated) ──────────────────────────────────────────────
-- Denormalized table for fast analytics lookups.
CREATE TABLE IF NOT EXISTS skill_scores (
    user_id         TEXT    NOT NULL,
    skill           TEXT    NOT NULL,   -- often the 'topic'
    avg_score       REAL    NOT NULL,
    attempts        INTEGER NOT NULL DEFAULT 1,
    last_updated    TEXT    NOT NULL DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (user_id, skill),
    FOREIGN KEY (user_id) REFERENCES users(user_id)
);
"""


# ─────────────────────────────────────────────────────────────────────────────
# Connection
# ─────────────────────────────────────────────────────────────────────────────

def _get_conn() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA journal_mode=WAL")   # safe for concurrent reads
    conn.execute("PRAGMA foreign_keys=ON")
    return conn


def init_db() -> None:
    """Create tables if they don't exist. Safe to call on every startup."""
    with _get_conn() as conn:
        conn.executescript(SCHEMA)


# ─────────────────────────────────────────────────────────────────────────────
# User helpers
# ─────────────────────────────────────────────────────────────────────────────

def get_or_create_user(user_id: str, initial_difficulty: float = 5.0) -> float:
    """
    Returns the user's current_difficulty.
    Creates a new user row with `initial_difficulty` if they don't exist.
    """
    with _get_conn() as conn:
        row = conn.execute(
            "SELECT current_difficulty FROM users WHERE user_id = ?",
            (user_id,)
        ).fetchone()
        if row:
            return float(row["current_difficulty"])
        conn.execute(
            "INSERT INTO users (user_id, current_difficulty) VALUES (?, ?)",
            (user_id, initial_difficulty),
        )
        return initial_difficulty


def update_user_difficulty(user_id: str, new_difficulty: float) -> None:
    """Upsert the user's difficulty and increment their question counter."""
    with _get_conn() as conn:
        conn.execute("""
            INSERT INTO users (user_id, current_difficulty, questions_answered, updated_at)
            VALUES (?, ?, 1, CURRENT_TIMESTAMP)
            ON CONFLICT(user_id) DO UPDATE SET
                current_difficulty  = excluded.current_difficulty,
                questions_answered  = questions_answered + 1,
                updated_at          = CURRENT_TIMESTAMP
        """, (user_id, new_difficulty))


# ─────────────────────────────────────────────────────────────────────────────
# Question helpers
# ─────────────────────────────────────────────────────────────────────────────

def save_question(
    question_id: str,
    topic: str,
    q: dict,
    difficulty: float,
    expected_time_ms: int,
) -> None:
    """
    Persist a Groq-generated question.
    Silently ignores duplicate question_ids (INSERT OR IGNORE).
    """
    with _get_conn() as conn:
        conn.execute("""
            INSERT OR IGNORE INTO questions
                (question_id, topic, question_text, options, correct_answer,
                 difficulty, expected_time_ms)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            question_id,
            topic,
            q["question"],
            json.dumps(q["options"]),
            q["correct"],
            difficulty,
            expected_time_ms,
        ))


def get_question(question_id: str) -> dict | None:
    """Return a question row as a plain dict, or None if not found."""
    with _get_conn() as conn:
        row = conn.execute(
            "SELECT * FROM questions WHERE question_id = ?",
            (question_id,)
        ).fetchone()
        if not row:
            return None
        d = dict(row)
        d["options"] = json.loads(d["options"])
        return d


def get_nearest_question(
    target_difficulty: float,
    exclude_id: str,
    topic: str | None = None,
) -> dict | None:
    """
    Find the stored question whose difficulty is closest to target_difficulty,
    excluding the question the user just answered.
    Optionally filters by topic.
    """
    with _get_conn() as conn:
        if topic:
            row = conn.execute("""
                SELECT * FROM questions
                WHERE question_id != ?
                  AND topic = ?
                ORDER BY ABS(difficulty - ?) ASC
                LIMIT 1
            """, (exclude_id, topic, target_difficulty)).fetchone()
        else:
            row = conn.execute("""
                SELECT * FROM questions
                WHERE question_id != ?
                ORDER BY ABS(difficulty - ?) ASC
                LIMIT 1
            """, (exclude_id, target_difficulty)).fetchone()

        if not row:
            return None
        d = dict(row)
        d["options"] = json.loads(d["options"])
        return d


# ─────────────────────────────────────────────────────────────────────────────
# Interaction log
# ─────────────────────────────────────────────────────────────────────────────

def log_interaction(
    user_id: str,
    question_id: str,
    payload: dict,
    delta_d: float,
    new_difficulty: float,
) -> None:
    """Append one submission record to interaction_logs."""
    with _get_conn() as conn:
        conn.execute("""
            INSERT INTO interaction_logs
                (user_id, question_id, payload, delta_d, new_difficulty)
            VALUES (?, ?, ?, ?, ?)
        """, (user_id, question_id, json.dumps(payload), delta_d, new_difficulty))


# ─────────────────────────────────────────────────────────────────────────────
# Analytics & Sessions
# ─────────────────────────────────────────────────────────────────────────────

def save_assessment_session(
    user_id: str,
    assessment_type: str,
    topic: str,
    score: float,
    max_score: float,
    feedback: str = "",
    metadata: dict = None
) -> str:
    """Save a completed assessment and update skill scores."""
    if metadata and metadata.get('session_id'):
        session_id = metadata.get('session_id')
    else:
        session_id = str(uuid.uuid4())
    
    pct = (score / max_score * 100) if max_score > 0 else 0
    
    with _get_conn() as conn:
        # 1. Save session
        conn.execute("""
            INSERT INTO assessment_sessions 
            (session_id, user_id, assessment_type, topic, score, max_score, percentage, feedback, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (session_id, user_id, assessment_type, topic, score, max_score, pct, feedback, json.dumps(metadata or {})))
        
        # 2. Update Aggregated Skills
        conn.execute("""
            INSERT INTO skill_scores (user_id, skill, avg_score, attempts, last_updated)
            VALUES (?, ?, ?, 1, CURRENT_TIMESTAMP)
            ON CONFLICT(user_id, skill) DO UPDATE SET
                avg_score = (avg_score * attempts + excluded.avg_score) / (attempts + 1),
                attempts = attempts + 1,
                last_updated = CURRENT_TIMESTAMP
        """, (user_id, topic, pct))
        
    return session_id

def get_analytics_summary(user_id: str) -> dict:
    """Return comprehensive analytics for the dashboard."""
    with _get_conn() as conn:
        # Skill-wise breakdown
        skills = conn.execute(
            "SELECT skill, avg_score, attempts FROM skill_scores WHERE user_id = ? ORDER BY avg_score DESC",
            (user_id,)
        ).fetchall()
        
        # Recent sessions
        sessions = conn.execute(
            "SELECT * FROM assessment_sessions WHERE user_id = ? ORDER BY created_at DESC LIMIT 10",
            (user_id,)
        ).fetchall()
        
        # Overall stats
        stats = conn.execute("""
            SELECT 
                COUNT(*) as total_assessments,
                AVG(percentage) as avg_percentage,
                MAX(percentage) as best_percentage
            FROM assessment_sessions WHERE user_id = ?
        """, (user_id,)).fetchone()

        return {
            "skills": [dict(s) for s in skills],
            "sessions": [dict(s) for s in sessions],
            "overall": dict(stats) if stats else {}
        }
